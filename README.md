# My Little Minion (mylm)

[![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://www.rust-lang.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Terminal AI](https://img.shields.io/badge/Terminal-AI-blue.svg)](#)

> **The AI assistant that actually understands your terminal.** Built in Rust. Designed for developers who want real productivity, not just chat.

**mylm** is a **multi-agent terminal AI assistant** that goes beyond simple Q&A. It sees what you see, remembers your projects, delegates tasks to specialized sub-agents, and safely executes commandsâ€”all while keeping you in control.

![mylm Dashboard](assets/hero.png)

---

## ğŸš€ Why mylm vs. The Alternatives

Recent tools like **OpenClaw** proved there's massive demand for terminal AI assistants. But they also exposed critical flaws: fragile context capture, no memory across sessions, single-threaded reasoning, and limited tool ecosystems.

**mylm was built differently from the ground up:**

| Feature | mylm | Others |
|---------|------|--------|
| **Multi-Agent Architecture** | âœ… Orchestrator + worker agents | âŒ Single agent |
| **Local Vector Memory** | âœ… LanceDB with semantic search | âŒ No memory |
| **Parallel Consensus (PaCoRe)** | âœ… Multi-path reasoning | âŒ Single-shot |
| **Terminal-Native Context** | âœ… tmux integration + full capture | âš ï¸ Partial |
| **Background Jobs** | âœ… Async task scheduling | âŒ Blocking |
| **Safety System** | âœ… Allowlists + approval workflow | âš ï¸ Basic |
| **Speed** | âœ… Rust-native, sub-100ms startup | âš ï¸ Slower |

---

## âœ¨ What Makes mylm Special

### ğŸ¯ `ai pop` â€” Context Magic
Your command fails. Instead of copying error messages, just type:
```bash
ai pop
```
mylm captures your terminal history, working directory, git state, environment variables, and recent commands. The AI sees exactly what you see. **No setup. No copy-paste. Just context.**

*Requires tmux (we'll help you set it up).*

### ğŸ§  Multi-Agent System
Most AI assistants are a single brain trying to do everything. mylm uses an **orchestrator-worker pattern**:

- **Orchestrator** plans and delegates
- **Worker agents** execute subtasks in parallel
- **Delegate tool** spawns specialized agents with their own toolsets
- **Job registry** tracks progress across all agents

Research a library while refactoring codeâ€”all at once.

### ğŸ’¾ Local Vector Memory (LanceDB)
mylm doesn't forget. It stores:
- Project decisions and architecture notes
- Code patterns and preferences
- Conversation history (semantically searchable)
- File relationships and dependencies

Over time, it learns *your* codebase. Ask "How do we handle auth here?" and get relevant answers from past conversations.

### ğŸ”„ PaCoRe: Parallel Consensus Reasoning
When accuracy matters, mylm can run **multi-round reasoning**:
1. Spawn multiple parallel LLM calls with different reasoning paths
2. Let them critique and build on each other's answers
3. Synthesize a consensus response

Better answers for complex debugging and architecture decisions.

### ğŸ›¡ï¸ Safety-First Execution
Every command goes through:
1. **Static analysis** â€” Pattern-based risk detection
2. **Allowlist checking** â€” Known safe commands
3. **User approval** â€” You see it before it runs

Run with `--execute` for trusted commands. Use `--force` only when you know what you're doing.

### ğŸŒ 10+ Built-in Tools
- **shell** â€” Execute with safety checks
- **git** â€” Status, log, diff analysis
- **fs** â€” Read/write files
- **web_search** â€” Real-time information
- **crawl** â€” Deep documentation extraction
- **memory** â€” Store and retrieve knowledge
- **delegate** â€” Spawn sub-agents
- **state** â€” Persistent key-value storage
- **terminal_sight** â€” Capture terminal state
- **system** â€” Resource monitoring

### âš¡ Built for Speed
- **Rust** â€” Zero-cost abstractions, memory safety
- **Async tokio** â€” Non-blocking I/O throughout
- **Optimized profiles** â€” Fast compile in dev, LTO in release
- **Sub-100ms** cold start to interactive

---

## ğŸ¬ Quick Start

### Installation
```bash
git clone https://github.com/ediblackk/mylm.git
cd mylm
chmod +x install.sh
./install.sh
```

**No sudo required.** Installs to `~/.local/bin`.

### First Use
```bash
# Launch the hub
ai

# Quick question
ai "how do I find large files in this repo?"

# Pop terminal context (inside tmux)
 cargo build  # fails...
ai pop        # "What's wrong?"

# Interactive session
ai interactive
```

---

## ğŸ“š Core Commands

| Command | Description |
|---------|-------------|
| `ai` | Hub â€” start conversations, manage sessions, configure |
| `ai "question"` | One-shot query with context |
| `ai pop` | Pop terminal context into AI (tmux) |
| `ai interactive` | Full TUI session |
| `ai session list` | View saved sessions |
| `ai session resume <id>` | Continue a conversation |
| `ai config` | Settings dashboard |
| `ai --version` | Show version & build info |

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         CLI (ai)                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Hub    â”‚  â”‚  TUI     â”‚  â”‚ One-Shot â”‚  â”‚  Daemon    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                       mylm-core                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Agent V2 (Orchestrator)                  â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  Reason    â”‚â†’ â”‚   Plan     â”‚â†’ â”‚    Delegate    â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Tools   â”‚  â”‚  Memory  â”‚  â”‚  PaCoRe  â”‚  â”‚  Jobs    â”‚   â”‚
â”‚  â”‚ Registry â”‚  â”‚ VectorDB â”‚  â”‚  Engine  â”‚  â”‚Scheduler â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚         Context Engine (git, sys, terminal)          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              LLM Client (Multi-Provider)              â”‚  â”‚
â”‚  â”‚   Gemini Â· OpenAI Â· Anthropic Â· Ollama Â· DeepSeek    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Configuration

Config lives in `~/.config/mylm/mylm.yaml`:

```yaml
profile: default
profiles:
  default:
    provider: Gemini
    model: gemini-2.0-flash-exp
    api_key: "${GEMINI_API_KEY}"
    base_url: "https://generativelanguage.googleapis.com/v1beta"
    max_iterations: 50
    
features:
  memory:
    enabled: true
  web_search:
    enabled: true
    provider: searxng
```

Or use the interactive dashboard: `ai config`

---

## ğŸ”’ Security & Privacy

- **Local-first**: Vector DB runs locally (LanceDB)
- **No telemetry**: Your data stays yours
- **Command safety**: Approval workflow, allowlists, pattern detection
- **API key handling**: Stored in config, never logged
- **Sandboxed execution**: Commands run in isolated PTY

---

## ğŸ› ï¸ Supported Providers

**Local (Free, Private):**
- Ollama
- LM Studio
- HuggingFace (via inference API)

**Cloud (API Key Required):**
- Google Gemini
- OpenAI (GPT-4, GPT-3.5)
- Anthropic (Claude)
- DeepSeek
- StepFun
- Kimi (Moonshot)

---

## ğŸ§ª Advanced Features

### Batch Processing (PaCoRe)
```bash
# Run multi-round consensus on a dataset
ai batch --input questions.jsonl --output results.jsonl \
  --model gemini-2.0-flash-exp --rounds "3,2,1"
```

### Background Jobs
```bash
ai  # Hub â†’ Background Jobs
# View, monitor, and manage long-running tasks
```

### Custom Prompts
Edit per-profile prompts in `~/.config/mylm/prompts/`:
```bash
ai config edit prompt
```

---

## ğŸš§ Roadmap

- [x] Multi-agent architecture with delegation
- [x] Local vector memory with LanceDB
- [x] PaCoRe parallel consensus reasoning
- [x] Job scheduling and background execution
- [x] Session persistence and management
- [ ] MCP (Model Context Protocol) integration
- [ ] Plugin system for custom tools
- [ ] Web dashboard for job monitoring
- [ ] Team sharing for memory stores

---

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

**New to the project?** Check out [ONBOARDING.md](ONBOARDING.md) for a gentle introduction to the codebase.

---

## ğŸ™ Acknowledgements

Built on the shoulders of giants:
- **Rust** â€” For performance and safety
- **ratatui** â€” Beautiful terminal UIs
- **tokio** â€” Async runtime
- **LanceDB** â€” Vector search
- **Google, Anthropic, OpenAI, Meta** â€” For pushing AI forward

And countless open-source contributors. And coffee. â˜•

---

## ğŸ“„ License

MIT â€” See [LICENSE](LICENSE) for details.

---

<p align="center">
  <strong>Stop copying errors. Start <code>ai pop</code>.</strong>
</p>

<p align="center">
  <sub>Keywords: Terminal AI, CLI LLM, AI Agent, Multi-Agent System, Developer Productivity, Local LLM, Vector Memory, Rust CLI, tmux AI, Autonomous Coding Assistant</sub>
</p>
