# My Language Model

[![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://www.rust-lang.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Terminal AI](https://img.shields.io/badge/Terminal-AI-blue.svg)](.)

<table>
  <tr>
    <td align="center"><b>Basic Usage</b></td>
    <td align="center"><b>Chat Interface</b></td>
    <td align="center"><b>Terminal Integration</b></td>
  </tr>
  <tr>
    <td><img src="assets/1.gif" width="260"></td>
    <td><img src="assets/2.gif" width="260"></td>
    <td><img src="assets/3.gif" width="260"></td>
  </tr>
</table>


## Safe Personal AI assistance into your terminal

**MyLM** is a privacy-focused, multi-agent terminal AI assistant built in Rust. It can see in your terminal, remember your projects, delegates tasks to specialized sub-agents, and safely executes commandsâ€”all while keeping you in control.

### Features

- **Persistent vector memory** â€” LanceDB with semantic search across sessions
- **Multi-agent orchestration** â€” Parallel workers with job tracking
- **Full terminal context** â€” tmux integration, zero manual context sharing
- **Background job system** â€” Async delegation, non-blocking
- **Safety-first execution** â€” Allowlists, approval workflow, PTY isolation
- **Lightweight & fast** â€” Fast cold starts and quick execution

---

## Quick Start

### Installation

git clone https://github.com/ediblackk/mylm.git

cd mylm

chmod +x install.sh

./install.sh

Installs to `~/.local/bin` without sudo.

### First Use

Unless you changed the alias, simply type "ai" into your terminal to open the central hub where you can change your settings, start new sessions or resume old ones.

---

## âœ¨ What Makes mylm Special
Considering you have enabled tmux
### ğŸ¯ `ai pop` â€” Context Magic
mylm captures your terminal history, working directory, git state, environment variables, and recent commands. The AI sees exactly what you see. **No setup. No copy-paste. Just context.**

*Requires tmux (we'll help you set it up).*

### ğŸ§  Multi-Agent System
Most AI assistants are a single brain trying to do everything. mylm uses an **orchestrator-worker pattern**:

- **Orchestrator** chats, plans and delegates
- **Delegate tool** spawns worker agents with their own toolsets
- **Worker agents** execute subtasks in parallel
- **Job registry** tracks progress across all agents

### KNOWN ISSUES / SOLVING SOON
- **Context grows uncontrollably** - when simply conversating with some history, context can grow way more than required; most likely some repeated data/content not caching; 
- **On stall, workers are not approved to contine** - in order to prevent unending loops, wasted resources and precise actions, sub-agent workers require approval for more actions after a number of actions (15); as they reach it, main agent does not react and does not allow further actions;
- **Memory needs refining** - scribe function is currently disabled; it is suppossed to continuously memorise actions and information, and inject information relevant to context to make the main model aware; this adds complexity and for basic functions is a bit overkill; totally refining this later though;
- **Code organisation and cleanup** - I am aware there is still mess around in  codebase; my main concern was to finish it and have something fast, stable and usable in day to day tasks; so far I am glad with how it runs, it's my first Rust project and I love it.


### ğŸ”„ PaCoRe: Parallel Consensus Reasoning
(https://github.com/stepfun-ai/PaCoRe)
When accuracy matters, mylm can run **multi-round reasoning**:
1. Spawn multiple parallel LLM calls with different reasoning paths
2. Let them critique and build on each other's answers
3. Synthesize a consensus response

Better answers for complex debugging and architecture decisions.

### ğŸ›¡ï¸ Safety-First Execution
Every command goes through:
1. **Static analysis** â€” Pattern-based risk detection
2. **Allowlist checking** â€” Known safe commands
3. **User approval** â€” You see it before it runs


### ğŸŒ 10+ Built-in Tools
- **shell** â€” Execute with safety checks
- **git** â€” Status, log, diff analysis
- **fs** â€” Read/write files
- **web_search** â€” Real-time information
- **crawl** â€” Deep documentation extraction
- **memory** â€” Store and retrieve knowledge
- **delegate** â€” Spawn sub-agents
- **state** â€” Persistent key-value storage
- **terminal_sight** â€” Capture terminal state
- **system** â€” Resource monitoring

### âš¡ Built for Speed
- **Rust** â€” Zero-cost abstractions, memory safety
- **Async tokio** â€” Non-blocking I/O throughout
- **Optimized profiles** â€” Fast compile in dev, LTO in release

---

## ğŸ”’ Security & Privacy

- **Local-first**: Vector DB runs locally (LanceDB)
- **No telemetry**: Your data stays yours
- **Command safety**: Approval workflow, allowlists, pattern detection
- **API key handling**: Stored in config, never logged
- **Sandboxed execution**: Commands run in isolated PTY

---

## ğŸ› ï¸ Supported Providers

**Local (Free, Private):**
- Ollama
- LM Studio
- HuggingFace (via inference API)

**Cloud (API Key Required):**
- Any OpenAI Compatible endpoint
- Google Gemini 3
- OpenAI (GPT-5.2)
- Anthropic (Claude Sonnet 4.5)
- DeepSeek V3.2
- StepFun 3.5 Flash
- Kimi K2.5 (Moonshot)

---

## ğŸ™ Acknowledgements

Built with assistance from every AI that would talk to me: Claude, GPT, Gemini,
Kimi, DeepSeek, StepFun, Z.AI, MiniMax and probably others I've forgotten. Also 147 Rust crates I didn't write. And Linux/Debian.And VS Code. And ASUS. And liters of coffee.

---

## ğŸ“„ License

MIT â€” See [LICENSE](LICENSE) for details.